{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SDE Head + SDE Evolution Block\n",
    "\n",
    "This notebook trains a model that pairs the **SDEEvolutionBlock** backbone with the\n",
    "**SDEHead** (and optionally the **NeuralSDEHead**) for price-path generation.\n",
    "\n",
    "- **SDEEvolutionBlock** learns residual stochastic updates inside the backbone,\n",
    "  giving the latent representation a diffusion-like inductive bias.\n",
    "- **SDEHead** maps the backbone output to `(mu, sigma)` drift/volatility parameters\n",
    "  that drive a standard GBM path simulation.\n",
    "- **NeuralSDEHead** goes further by learning full drift/diffusion *networks* and\n",
    "  integrating them via `torchsde.sdeint` for numerically stable paths.\n",
    "\n",
    "1. Load OHLCV data from `tensorlink-dev/open-synth-training-data`\n",
    "2. Engineer 16 micro-structure features per 1-hour bar\n",
    "3. Build a backbone using **TransformerBlock + SDEEvolutionBlock**\n",
    "4. Attach an **SDEHead** (or **NeuralSDEHead**)\n",
    "5. Train with CRPS loss and backtest with multi-interval scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.registry import discover_components, registry, SDEEvolutionBlock, TransformerBlock\n",
    "from src.models.factory import HybridBackbone, SynthModel\n",
    "from src.models.heads import SDEHead, NeuralSDEHead\n",
    "from src.data.market_data_loader import (\n",
    "    HFOHLCVSource,\n",
    "    MockDataSource,\n",
    "    OHLCVEngineer,\n",
    "    OHLCV_FEATURE_NAMES,\n",
    "    MarketDataLoader,\n",
    ")\n",
    "from src.research.trainer import Trainer, DataToModelAdapter\n",
    "from src.research.metrics import (\n",
    "    crps_ensemble,\n",
    "    CRPSMultiIntervalScorer,\n",
    "    SCORING_INTERVALS,\n",
    ")\n",
    "\n",
    "discover_components(\"src/models/components\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"OHLCV feature count: {len(OHLCV_FEATURE_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data config -----\n",
    "REPO_ID = \"tensorlink-dev/open-synth-training-data\"\n",
    "ASSET_FILES = {\n",
    "    \"BTC_USD\": \"data/BTC_USD/5m.parquet\",\n",
    "    \"ETH_USD\": \"data/ETH_USD/5m.parquet\",\n",
    "    \"SOL_USD\": \"data/SOL_USD/5m.parquet\",\n",
    "}\n",
    "ASSETS = list(ASSET_FILES.keys())\n",
    "USE_HF = True\n",
    "\n",
    "INPUT_LEN = 64\n",
    "PRED_LEN = 12\n",
    "BATCH_SIZE = 8\n",
    "FEATURE_DIM = 16\n",
    "\n",
    "# ----- Model config -----\n",
    "D_MODEL = 64\n",
    "SDE_BLOCK_HIDDEN = 128   # Hidden dim for SDEEvolutionBlock\n",
    "SDE_HEAD_HIDDEN = 64     # Hidden dim for the SDE head MLPs\n",
    "N_PATHS = 500\n",
    "\n",
    "# ----- Head selection -----\n",
    "# \"sde\"        -> SDEHead  (outputs mu, sigma for GBM simulation)\n",
    "# \"neural_sde\" -> NeuralSDEHead (learns drift/diffusion networks, integrates via torchsde)\n",
    "HEAD_TYPE = \"sde\"\n",
    "\n",
    "# NeuralSDEHead-specific config\n",
    "NEURAL_SDE_SOLVER = \"euler\"   # \"euler\", \"milstein\", or \"srk\"\n",
    "NEURAL_SDE_ADJOINT = False    # Use adjoint method for memory-efficient backprop\n",
    "\n",
    "# ----- Training config -----\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "\n",
    "# ----- Backtest config -----\n",
    "TIME_INCREMENT = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = OHLCVEngineer(resample_rule=\"1h\")\n",
    "\n",
    "if USE_HF:\n",
    "    source = HFOHLCVSource(\n",
    "        repo_id=REPO_ID,\n",
    "        asset_files=ASSET_FILES,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "else:\n",
    "    source = MockDataSource(length=8000, freq=\"5min\", seed=42, base_price=100.0)\n",
    "\n",
    "loader = MarketDataLoader(\n",
    "    data_source=source,\n",
    "    engineer=engineer,\n",
    "    assets=ASSETS,\n",
    "    input_len=INPUT_LEN,\n",
    "    pred_len=PRED_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_dim=FEATURE_DIM,\n",
    ")\n",
    "\n",
    "print(f\"Assets loaded:  {[a.name for a in loader.assets_data]}\")\n",
    "print(f\"Total windows:  {len(loader.dataset)}\")\n",
    "print(f\"Sample input shape (F, T): {loader.dataset[0]['inputs'].shape}\")\n",
    "print(f\"Sample target shape (1, T): {loader.dataset[0]['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = loader.static_holdout(\n",
    "    cutoff=0.2,\n",
    "    val_size=0.15,\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dl)}\")\n",
    "print(f\"Val batches:   {len(val_dl)}\")\n",
    "print(f\"Test batches:  {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Build the Model\n",
    "\n",
    "### Backbone: Transformer + SDE Evolution Block\n",
    "\n",
    "The backbone stacks a **TransformerBlock** (multi-head self-attention + gated MLP)\n",
    "followed by an **SDEEvolutionBlock** (residual stochastic update: `x + MLP(x)`).\n",
    "\n",
    "The SDEEvolutionBlock introduces a diffusion-like inductive bias into the latent\n",
    "representation, learning to add stochastic-style perturbations that complement\n",
    "the deterministic attention patterns.\n",
    "\n",
    "### Head: SDEHead or NeuralSDEHead\n",
    "\n",
    "- **SDEHead**: 2-layer MLP mapping `h_t -> (mu, sigma)`. Paths are simulated\n",
    "  externally via standard GBM (Euler-Maruyama).\n",
    "- **NeuralSDEHead**: Learns drift `f(t, y | ctx)` and diffusion `g(t, y | ctx)`\n",
    "  networks that are integrated using `torchsde.sdeint` in log-price space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    TransformerBlock(d_model=D_MODEL, nhead=4, dropout=0.1),\n",
    "    SDEEvolutionBlock(d_model=D_MODEL, hidden=SDE_BLOCK_HIDDEN, dropout=0.1),\n",
    "    TransformerBlock(d_model=D_MODEL, nhead=4, dropout=0.1),\n",
    "    SDEEvolutionBlock(d_model=D_MODEL, hidden=SDE_BLOCK_HIDDEN, dropout=0.1),\n",
    "]\n",
    "\n",
    "backbone = HybridBackbone(\n",
    "    input_size=FEATURE_DIM,\n",
    "    d_model=D_MODEL,\n",
    "    blocks=blocks,\n",
    "    validate_shapes=True,\n",
    ")\n",
    "\n",
    "if HEAD_TYPE == \"neural_sde\":\n",
    "    head = NeuralSDEHead(\n",
    "        latent_size=backbone.output_dim,\n",
    "        hidden=SDE_HEAD_HIDDEN,\n",
    "        solver=NEURAL_SDE_SOLVER,\n",
    "        adjoint=NEURAL_SDE_ADJOINT,\n",
    "    )\n",
    "    head_label = f\"NeuralSDEHead (solver={NEURAL_SDE_SOLVER}, adjoint={NEURAL_SDE_ADJOINT})\"\n",
    "else:\n",
    "    head = SDEHead(\n",
    "        latent_size=backbone.output_dim,\n",
    "        hidden=SDE_HEAD_HIDDEN,\n",
    "    )\n",
    "    head_label = f\"SDEHead (hidden={SDE_HEAD_HIDDEN})\"\n",
    "\n",
    "model = SynthModel(backbone=backbone, head=head).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "backbone_params = sum(p.numel() for p in backbone.parameters())\n",
    "head_params = sum(p.numel() for p in head.parameters())\n",
    "print(f\"SynthModel with {head_label} built successfully\")\n",
    "print(f\"  Backbone: TransformerBlock + SDEEvolutionBlock (x2)\")\n",
    "print(f\"  Backbone output dim: {backbone.output_dim}\")\n",
    "print(f\"  Backbone params:     {backbone_params:,}\")\n",
    "print(f\"  Head params:         {head_params:,}\")\n",
    "print(f\"  Total parameters:    {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Sanity Check: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.randn(2, INPUT_LEN, FEATURE_DIM, device=device)\n",
    "dummy_price = torch.tensor([100.0, 50.0], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    paths, mu, sigma = model(dummy_x, initial_price=dummy_price, horizon=PRED_LEN, n_paths=100)\n",
    "\n",
    "print(f\"Paths shape:  {paths.shape}   (batch, n_paths, horizon)\")\n",
    "print(f\"Mu shape:     {mu.shape}\")\n",
    "print(f\"Sigma shape:  {sigma.shape}\")\n",
    "print(f\"Mu values:    {mu.cpu().numpy()}\")\n",
    "print(f\"Sigma values: {sigma.cpu().numpy()}\")\n",
    "print(f\"Path range:   [{paths.min().item():.4f}, {paths.max().item():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "adapter = DataToModelAdapter(device=device, target_is_log_return=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    n_paths=N_PATHS,\n",
    "    device=device,\n",
    "    adapter=adapter,\n",
    ")\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_crps\": [],\n",
    "    \"train_sharpness\": [],\n",
    "    \"val_crps\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_losses = []\n",
    "    epoch_crps = []\n",
    "    epoch_sharp = []\n",
    "\n",
    "    for batch in train_dl:\n",
    "        metrics = trainer.train_step(batch)\n",
    "        epoch_losses.append(metrics[\"loss\"])\n",
    "        epoch_crps.append(metrics[\"crps\"])\n",
    "        epoch_sharp.append(metrics[\"sharpness\"])\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    avg_crps = np.mean(epoch_crps)\n",
    "    avg_sharp = np.mean(epoch_sharp)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"train_crps\"].append(avg_crps)\n",
    "    history[\"train_sharpness\"].append(avg_sharp)\n",
    "\n",
    "    val_metrics = trainer.validate(val_dl)\n",
    "    history[\"val_crps\"].append(val_metrics[\"val_crps\"])\n",
    "\n",
    "    if epoch % 3 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{EPOCHS}  \"\n",
    "            f\"train_loss={avg_loss:.5f}  \"\n",
    "            f\"train_crps={avg_crps:.5f}  \"\n",
    "            f\"val_crps={val_metrics['val_crps']:.5f}  \"\n",
    "            f\"sharpness={avg_sharp:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss (CRPS)\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history[\"train_crps\"], label=\"Train CRPS\")\n",
    "axes[1].plot(history[\"val_crps\"], label=\"Val CRPS\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"CRPS\")\n",
    "axes[1].set_title(\"CRPS: Train vs Validation\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history[\"train_sharpness\"], label=\"Sharpness\", color=\"tab:green\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Std(paths)\")\n",
    "axes[2].set_title(\"Forecast Sharpness\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Backtesting on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scorer = CRPSMultiIntervalScorer(time_increment=TIME_INCREMENT)\n",
    "\n",
    "interval_scores = {name: [] for name in SCORING_INTERVALS}\n",
    "overall_scores = []\n",
    "all_test_crps = []\n",
    "\n",
    "for batch in test_dl:\n",
    "    adapted = adapter(batch)\n",
    "    history_t = adapted[\"history\"]\n",
    "    initial_price = adapted[\"initial_price\"]\n",
    "    target_factors = adapted[\"target_factors\"]\n",
    "    horizon = target_factors.shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        paths, mu, sigma = model(\n",
    "            history_t,\n",
    "            initial_price=initial_price,\n",
    "            horizon=horizon,\n",
    "            n_paths=N_PATHS,\n",
    "        )\n",
    "\n",
    "    sim_paths = paths.transpose(1, 2)\n",
    "    crps_vals = crps_ensemble(sim_paths, target_factors)\n",
    "    all_test_crps.append(crps_vals.mean().item())\n",
    "\n",
    "    for sample_idx in range(paths.shape[0]):\n",
    "        total_crps, detailed = scorer(paths[sample_idx], paths[sample_idx, 0])\n",
    "        overall_scores.append(total_crps)\n",
    "        for row in detailed:\n",
    "            interval_name = row[\"Interval\"]\n",
    "            if interval_name in interval_scores and row[\"Increment\"] == \"Total\":\n",
    "                interval_scores[interval_name].append(float(row[\"CRPS\"]))\n",
    "\n",
    "avg_test_crps = np.mean(all_test_crps)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BACKTEST RESULTS -- SDE Head + SDE Block\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Head type:         {HEAD_TYPE}\")\n",
    "print(f\"Average Test CRPS: {avg_test_crps:.6f}\")\n",
    "print(f\"\\nMulti-Interval CRPS Breakdown:\")\n",
    "for name, scores in interval_scores.items():\n",
    "    if scores:\n",
    "        print(f\"  {name:>12s}: {np.mean(scores):.6f} (n={len(scores)})\")\n",
    "    else:\n",
    "        print(f\"  {name:>12s}: N/A (horizon too short)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Fan Chart Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_batch = next(iter(test_dl))\n",
    "adapted = adapter(test_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    paths, mu, sigma = model(\n",
    "        adapted[\"history\"],\n",
    "        initial_price=adapted[\"initial_price\"],\n",
    "        horizon=adapted[\"target_factors\"].shape[-1],\n",
    "        n_paths=N_PATHS,\n",
    "    )\n",
    "\n",
    "paths_np = paths.cpu().numpy()\n",
    "targets_np = adapted[\"target_factors\"].cpu().numpy()\n",
    "\n",
    "n_show = min(4, paths_np.shape[0])\n",
    "fig, axes = plt.subplots(1, n_show, figsize=(5 * n_show, 4), squeeze=False)\n",
    "\n",
    "for i in range(n_show):\n",
    "    ax = axes[0, i]\n",
    "    sample_paths = paths_np[i]\n",
    "    t = np.arange(sample_paths.shape[1])\n",
    "\n",
    "    p5 = np.percentile(sample_paths, 5, axis=0)\n",
    "    p25 = np.percentile(sample_paths, 25, axis=0)\n",
    "    p50 = np.percentile(sample_paths, 50, axis=0)\n",
    "    p75 = np.percentile(sample_paths, 75, axis=0)\n",
    "    p95 = np.percentile(sample_paths, 95, axis=0)\n",
    "\n",
    "    ax.fill_between(t, p5, p95, alpha=0.15, color=\"tab:blue\", label=\"P5-P95\")\n",
    "    ax.fill_between(t, p25, p75, alpha=0.3, color=\"tab:blue\", label=\"P25-P75\")\n",
    "    ax.plot(t, p50, color=\"tab:blue\", linewidth=2, label=\"Median\")\n",
    "    ax.plot(t, targets_np[i], color=\"tab:red\", linewidth=2, linestyle=\"--\", label=\"Actual\")\n",
    "\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.set_xlabel(\"Horizon Step\")\n",
    "    ax.set_ylabel(\"Price Factor\")\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(f\"SDE Head + SDE Block Fan Charts (head={HEAD_TYPE})\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Path Distribution & Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Terminal price distribution for sample 0\n",
    "terminal_prices = paths_np[0, :, -1]\n",
    "axes[0].hist(terminal_prices, bins=50, alpha=0.7, color=\"tab:blue\", edgecolor=\"white\")\n",
    "axes[0].axvline(targets_np[0, -1], color=\"tab:red\", linewidth=2, linestyle=\"--\", label=\"Actual\")\n",
    "axes[0].set_title(\"Terminal Price Distribution\")\n",
    "axes[0].set_xlabel(\"Price Factor\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Collect mu and sigma across multiple batches\n",
    "model.eval()\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "for batch in test_dl:\n",
    "    adapted_viz = adapter(batch)\n",
    "    with torch.no_grad():\n",
    "        _, mu_viz, sigma_viz = model(\n",
    "            adapted_viz[\"history\"],\n",
    "            initial_price=adapted_viz[\"initial_price\"],\n",
    "            horizon=adapted_viz[\"target_factors\"].shape[-1],\n",
    "            n_paths=10,\n",
    "        )\n",
    "    all_mu.append(mu_viz.cpu().numpy().flatten())\n",
    "    all_sigma.append(sigma_viz.cpu().numpy().flatten())\n",
    "\n",
    "all_mu = np.concatenate(all_mu)\n",
    "all_sigma = np.concatenate(all_sigma)\n",
    "\n",
    "axes[1].hist(all_mu, bins=50, alpha=0.7, color=\"tab:orange\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Learned Drift (mu) Distribution\")\n",
    "axes[1].set_xlabel(\"mu\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "axes[2].hist(all_sigma, bins=50, alpha=0.7, color=\"tab:green\", edgecolor=\"white\")\n",
    "axes[2].set_title(\"Learned Volatility (sigma) Distribution\")\n",
    "axes[2].set_xlabel(\"sigma\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Drift   -- mean: {all_mu.mean():.6f}, std: {all_mu.std():.6f}\")\n",
    "print(f\"Vol     -- mean: {all_sigma.mean():.6f}, std: {all_sigma.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Compare SDEHead vs NeuralSDEHead (Optional)\n",
    "\n",
    "Re-run the notebook with `HEAD_TYPE = \"neural_sde\"` to compare:\n",
    "\n",
    "| Aspect | SDEHead | NeuralSDEHead |\n",
    "|--------|---------|---------------|\n",
    "| Output | `(mu, sigma)` scalars | Full paths via `torchsde.sdeint` |\n",
    "| Simulation | External GBM (constant params) | Internal SDE integration (state-dependent) |\n",
    "| Dynamics | Constant drift/vol per sample | Time-varying, state-dependent drift/vol |\n",
    "| Speed | Faster (no ODE solve) | Slower (numerical integration) |\n",
    "| Expressiveness | Lower (constant parameters) | Higher (neural drift and diffusion) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}