{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk-Forward DLinear with Regime Detection\n",
    "\n",
    "This notebook demonstrates how to use the walk-forward validation framework with regime detection to train and evaluate DLinear models on crypto market data.\n",
    "\n",
    "## Key Components:\n",
    "- **Walk-Forward Validation**: Creates chronological folds with train/val/test splits\n",
    "- **Regime Detection**: Clusters market conditions (e.g., high/low volatility) using K-Means\n",
    "- **DLinear Model**: Decomposition-based time series forecasting\n",
    "- **Per-Fold Training**: Trains separate models on each fold to simulate realistic deployment\n",
    "\n",
    "## Pipeline Overview:\n",
    "```\n",
    "Raw 5m OHLCV → Aggregate to 1h → Engineer Features → Detect Regimes → \n",
    "Generate Walk-Forward Folds → Train DLinear on Each Fold → Evaluate Performance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Commented out IPython magic to ensure Python compatibility.\n# 2. Clone the repository\n!git clone https://github.com/tensorlink-dev/open-synth-miner\n# %cd open-synth-miner\n!uv pip install torchsde\n# 3. Install dependencies using uv\n# --system: Installs into the Colab runtime (no venv needed)\n# -e .: Installs the package in editable mode\n!uv pip install --system -e .\n\n# 4. (Optional) Verify installation\n!python main.py --help",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Commented out IPython magic to ensure Python compatibility.\n# %cd open-synth-miner\n!git pull\n!uv pip install --system -e .",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Walk-Forward and Regime Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.regime_loader import (\n",
    "    load_raw_data,\n",
    "    aggregate_5m_to_1h,\n",
    "    engineer_features,\n",
    "    RegimeTagger,\n",
    "    KMeansStrategy,\n",
    "    GaussianMixtureStrategy,\n",
    "    generate_walk_forward_folds,\n",
    "    RegimeBalancedSampler,\n",
    "    RegimeDriftMonitor,\n",
    "    PipelineConfig,\n",
    "    run_pipeline\n",
    ")\n",
    "\n",
    "from src.models.components.advanced_blocks import DLinearBlock\n",
    "from src.models.components.backbones import HybridBackbone\n",
    "from src.models.heads import HorizonHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "We'll load raw 5-minute OHLCV data and process it through the regime detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw 5-minute data\n",
    "# Option 1: From HuggingFace (recommended)\n",
    "df_raw = load_raw_data(\n",
    "    repo_id=\"tensorlink-dev/open-synth-training-data\",\n",
    "    asset=\"BTC_USD\",\n",
    "    date_range=(\"2023-01-01\", \"2024-12-31\")\n",
    ")\n",
    "\n",
    "# Option 2: From local file\n",
    "# df_raw = pd.read_parquet('data/BTC_USD/5m.parquet')\n",
    "# df_raw.index = pd.to_datetime(df_raw.index)\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} bars from {df_raw.index[0]} to {df_raw.index[-1]}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Walk-Forward Pipeline\n",
    "\n",
    "Set up the pipeline parameters for:\n",
    "- Aggregation (5m → 1h)\n",
    "- Feature engineering (volatility, momentum, microstructure)\n",
    "- Regime detection (K-Means clustering)\n",
    "- Walk-forward fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "config = PipelineConfig(\n",
    "    # Fold sizes (in hours for 1h bars)\n",
    "    train_size=720,      # 30 days\n",
    "    val_size=168,        # 7 days\n",
    "    test_size=168,       # 7 days\n",
    "    step_size=168,       # Slide forward by 7 days\n",
    "    gap_size=24,         # 1-day gap between train/val and val/test\n",
    "    \n",
    "    # Model input/output dimensions\n",
    "    seq_len=64,          # 64 hours lookback (2.67 days)\n",
    "    pred_len=12,         # 12 hours forecast (0.5 days)\n",
    "    \n",
    "    # Regime detection\n",
    "    n_regimes=3,         # Low/Medium/High volatility\n",
    "    regime_features=['realized_vol', 'parkinson_vol'],\n",
    "    clustering_strategy='kmeans',  # or 'gmm'\n",
    "    \n",
    "    # DataLoader settings\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    use_regime_balancing=True,  # Oversample minority regimes\n",
    ")\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(f\"  Train: {config.train_size}h ({config.train_size/24:.1f} days)\")\n",
    "print(f\"  Val:   {config.val_size}h ({config.val_size/24:.1f} days)\")\n",
    "print(f\"  Test:  {config.test_size}h ({config.test_size/24:.1f} days)\")\n",
    "print(f\"  Gap:   {config.gap_size}h ({config.gap_size/24:.1f} days)\")\n",
    "print(f\"  Step:  {config.step_size}h ({config.step_size/24:.1f} days)\")\n",
    "print(f\"  Regimes: {config.n_regimes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Pipeline to Generate Folds\n",
    "\n",
    "This will:\n",
    "1. Aggregate 5m → 1h bars\n",
    "2. Engineer 13 features (volatility, momentum, microstructure)\n",
    "3. Detect regimes using K-Means clustering\n",
    "4. Generate walk-forward folds with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full pipeline\n",
    "fold_loaders = run_pipeline(df_raw, config)\n",
    "\n",
    "print(f\"\\nGenerated {len(fold_loaders)} walk-forward folds\\n\")\n",
    "\n",
    "# Inspect first fold\n",
    "for i, fold_loader in enumerate(fold_loaders[:3]):  # Show first 3 folds\n",
    "    train_dl, val_dl, test_dl = fold_loader.train_dl, fold_loader.val_dl, fold_loader.test_dl\n",
    "    \n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  Train: {len(train_dl.dataset)} samples\")\n",
    "    print(f\"  Val:   {len(val_dl.dataset)} samples\")\n",
    "    print(f\"  Test:  {len(test_dl.dataset)} samples\")\n",
    "    \n",
    "    # Show regime distribution\n",
    "    if hasattr(fold_loader, 'regime_distribution'):\n",
    "        dist = fold_loader.regime_distribution()\n",
    "        print(f\"  Regime distribution: {dist}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Regime Detection\n",
    "\n",
    "Plot the detected regimes over time to understand market state transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regime labels from first fold for visualization\n",
    "first_fold = fold_loaders[0]\n",
    "df_first = first_fold.df_train  # DataFrame with timestamp index\n",
    "\n",
    "if 'regime' in df_first.columns:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 10), sharex=True)\n",
    "    \n",
    "    # Plot 1: Price with regime background\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(df_first.index, df_first['close'], color='black', linewidth=1)\n",
    "    \n",
    "    # Color background by regime\n",
    "    regime_colors = {0: 'green', 1: 'yellow', 2: 'red'}\n",
    "    for regime_id in df_first['regime'].unique():\n",
    "        regime_mask = df_first['regime'] == regime_id\n",
    "        ax1.fill_between(\n",
    "            df_first.index, \n",
    "            df_first['close'].min(), \n",
    "            df_first['close'].max(),\n",
    "            where=regime_mask,\n",
    "            alpha=0.2,\n",
    "            color=regime_colors.get(regime_id, 'gray'),\n",
    "            label=f'Regime {regime_id}'\n",
    "        )\n",
    "    \n",
    "    ax1.set_ylabel('Close Price')\n",
    "    ax1.set_title('Price with Regime Background (Fold 1 Training Data)')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Realized Volatility\n",
    "    ax2 = axes[1]\n",
    "    if 'realized_vol' in df_first.columns:\n",
    "        ax2.plot(df_first.index, df_first['realized_vol'], color='purple', linewidth=1)\n",
    "        ax2.set_ylabel('Realized Vol')\n",
    "        ax2.set_title('Realized Volatility Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Regime timeline\n",
    "    ax3 = axes[2]\n",
    "    ax3.scatter(df_first.index, df_first['regime'], c=df_first['regime'], \n",
    "                cmap='RdYlGn_r', s=10, alpha=0.6)\n",
    "    ax3.set_ylabel('Regime ID')\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_title('Regime Timeline')\n",
    "    ax3.set_yticks([0, 1, 2])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No regime column found in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build DLinear Model\n",
    "\n",
    "Create a DLinear model with:\n",
    "- Trend-seasonal decomposition\n",
    "- Separate linear projections for each component\n",
    "- Multi-horizon forecasting head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dlinear_model(input_size, d_model=64, kernel_size=25, pred_len=12, num_paths=1):\n",
    "    \"\"\"Create a DLinear model with specified architecture.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        d_model: Hidden dimension size\n",
    "        kernel_size: Moving average kernel for decomposition\n",
    "        pred_len: Forecast horizon length\n",
    "        num_paths: Number of simulation paths (1 for point forecast)\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Complete DLinear model\n",
    "    \"\"\"\n",
    "    # Build backbone with stacked DLinear blocks\n",
    "    blocks = [\n",
    "        DLinearBlock(d_model=d_model, kernel_size=kernel_size),\n",
    "        DLinearBlock(d_model=d_model, kernel_size=kernel_size),\n",
    "    ]\n",
    "    \n",
    "    backbone = HybridBackbone(\n",
    "        input_size=input_size,\n",
    "        d_model=d_model,\n",
    "        blocks=blocks,\n",
    "    )\n",
    "    \n",
    "    # Add forecasting head\n",
    "    head = HorizonHead(\n",
    "        d_model=d_model,\n",
    "        pred_len=pred_len,\n",
    "        num_paths=num_paths,\n",
    "    )\n",
    "    \n",
    "    class DLinearModel(nn.Module):\n",
    "        def __init__(self, backbone, head):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            self.head = head\n",
    "        \n",
    "        def forward(self, x):\n",
    "            features = self.backbone(x)\n",
    "            return self.head(features)\n",
    "    \n",
    "    return DLinearModel(backbone, head)\n",
    "\n",
    "# Get input size from first batch\n",
    "sample_batch = next(iter(fold_loaders[0].train_dl))\n",
    "input_size = sample_batch['features'].shape[-1]\n",
    "print(f\"Input features: {input_size}\")\n",
    "print(f\"Prediction horizon: {config.pred_len}\")\n",
    "\n",
    "# Create model\n",
    "model = create_dlinear_model(\n",
    "    input_size=input_size,\n",
    "    d_model=64,\n",
    "    kernel_size=25,\n",
    "    pred_len=config.pred_len,\n",
    "    num_paths=1\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function\n",
    "\n",
    "Define training loop with:\n",
    "- MSE loss (can be replaced with CRPS for probabilistic forecasts)\n",
    "- Adam optimizer\n",
    "- Validation monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, epochs=10, lr=1e-3, patience=3):\n",
    "    \"\"\"Train model on a single fold.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_dl: Training DataLoader\n",
    "        val_dl: Validation DataLoader\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        patience: Early stopping patience\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history\n",
    "    \"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False):\n",
    "            features = batch['features'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            \n",
    "            # Handle potential shape mismatch\n",
    "            if predictions.dim() == 3 and targets.dim() == 2:\n",
    "                predictions = predictions.squeeze(-1)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dl, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False):\n",
    "                features = batch['features'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                predictions = model(features)\n",
    "                \n",
    "                if predictions.dim() == 3 and targets.dim() == 2:\n",
    "                    predictions = predictions.squeeze(-1)\n",
    "                \n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Walk-Forward Training Loop\n",
    "\n",
    "Train a separate model on each fold and evaluate on the test set.\n",
    "This simulates a realistic deployment scenario where models are retrained periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for each fold\n",
    "fold_results = []\n",
    "\n",
    "# Train on each fold (limit to first 5 folds for demo)\n",
    "num_folds = min(5, len(fold_loaders))\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold_idx + 1} / {num_folds}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    fold_loader = fold_loaders[fold_idx]\n",
    "    \n",
    "    # Create fresh model for this fold\n",
    "    model = create_dlinear_model(\n",
    "        input_size=input_size,\n",
    "        d_model=64,\n",
    "        kernel_size=25,\n",
    "        pred_len=config.pred_len,\n",
    "        num_paths=1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(\n",
    "        model,\n",
    "        fold_loader.train_dl,\n",
    "        fold_loader.val_dl,\n",
    "        epochs=15,\n",
    "        lr=1e-3,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in fold_loader.test_dl:\n",
    "            features = batch['features'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            predictions = model(features)\n",
    "            \n",
    "            if predictions.dim() == 3 and targets.dim() == 2:\n",
    "                predictions = predictions.squeeze(-1)\n",
    "            \n",
    "            loss = nn.MSELoss()(predictions, targets)\n",
    "            test_losses.append(loss.item())\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    test_loss = np.mean(test_losses)\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "    rmse = np.sqrt(np.mean((all_predictions - all_targets) ** 2))\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"  MSE:  {test_loss:.6f}\")\n",
    "    print(f\"  MAE:  {mae:.6f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WALK-FORWARD TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Aggregate Results Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "results_df = pd.DataFrame([{\n",
    "    'Fold': r['fold'],\n",
    "    'Test MSE': r['test_loss'],\n",
    "    'Test MAE': r['mae'],\n",
    "    'Test RMSE': r['rmse']\n",
    "} for r in fold_results])\n",
    "\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAggregate Statistics:\")\n",
    "print(f\"  Mean Test MSE:  {results_df['Test MSE'].mean():.6f} ± {results_df['Test MSE'].std():.6f}\")\n",
    "print(f\"  Mean Test MAE:  {results_df['Test MAE'].mean():.6f} ± {results_df['Test MAE'].std():.6f}\")\n",
    "print(f\"  Mean Test RMSE: {results_df['Test RMSE'].mean():.6f} ± {results_df['Test RMSE'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training curves for all folds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "for i, result in enumerate(fold_results):\n",
    "    history = result['history']\n",
    "    ax1.plot(history['train_loss'], label=f\"Fold {i+1} Train\", alpha=0.7)\n",
    "    ax1.plot(history['val_loss'], label=f\"Fold {i+1} Val\", linestyle='--', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Curves Across Folds')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test metrics comparison\n",
    "ax2 = axes[1]\n",
    "x = results_df['Fold']\n",
    "ax2.bar(x - 0.2, results_df['Test MSE'], width=0.2, label='MSE', alpha=0.8)\n",
    "ax2.bar(x, results_df['Test MAE'], width=0.2, label='MAE', alpha=0.8)\n",
    "ax2.bar(x + 0.2, results_df['Test RMSE'], width=0.2, label='RMSE', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Fold')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.set_title('Test Metrics by Fold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Prediction vs Actual for a sample fold\n",
    "sample_fold_idx = 0\n",
    "sample_result = fold_results[sample_fold_idx]\n",
    "\n",
    "# Take first 100 predictions for visualization\n",
    "n_samples = 100\n",
    "predictions = sample_result['predictions'][:n_samples]\n",
    "targets = sample_result['targets'][:n_samples]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Plot first horizon step\n",
    "ax1 = axes[0]\n",
    "ax1.plot(targets[:, 0], label='Actual', linewidth=2, alpha=0.8)\n",
    "ax1.plot(predictions[:, 0], label='Predicted', linewidth=2, alpha=0.8)\n",
    "ax1.set_ylabel('Price Return')\n",
    "ax1.set_title(f'Fold {sample_fold_idx + 1}: 1-Step Ahead Forecast (t+1)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot last horizon step\n",
    "ax2 = axes[1]\n",
    "ax2.plot(targets[:, -1], label='Actual', linewidth=2, alpha=0.8)\n",
    "ax2.plot(predictions[:, -1], label='Predicted', linewidth=2, alpha=0.8)\n",
    "ax2.set_xlabel('Sample Index')\n",
    "ax2.set_ylabel('Price Return')\n",
    "ax2.set_title(f'Fold {sample_fold_idx + 1}: {config.pred_len}-Step Ahead Forecast (t+{config.pred_len})')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Regime-Specific Performance Analysis\n",
    "\n",
    "Analyze model performance broken down by market regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by regime for first fold\n",
    "fold_loader = fold_loaders[0]\n",
    "model = create_dlinear_model(\n",
    "    input_size=input_size,\n",
    "    d_model=64,\n",
    "    kernel_size=25,\n",
    "    pred_len=config.pred_len,\n",
    "    num_paths=1\n",
    ").to(device)\n",
    "\n",
    "# Quick train\n",
    "history = train_model(model, fold_loader.train_dl, fold_loader.val_dl, epochs=10, lr=1e-3)\n",
    "\n",
    "# Evaluate by regime\n",
    "model.eval()\n",
    "regime_errors = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in fold_loader.test_dl:\n",
    "        features = batch['features'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        regimes = batch.get('regime', None)\n",
    "        \n",
    "        predictions = model(features)\n",
    "        \n",
    "        if predictions.dim() == 3 and targets.dim() == 2:\n",
    "            predictions = predictions.squeeze(-1)\n",
    "        \n",
    "        errors = torch.abs(predictions - targets).cpu().numpy()\n",
    "        \n",
    "        if regimes is not None:\n",
    "            regimes = regimes.cpu().numpy()\n",
    "            for regime_id in np.unique(regimes):\n",
    "                mask = regimes == regime_id\n",
    "                regime_errors.setdefault(regime_id, []).extend(errors[mask].flatten())\n",
    "\n",
    "# Plot regime-specific errors\n",
    "if regime_errors:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    regime_labels = [f\"Regime {k}\" for k in sorted(regime_errors.keys())]\n",
    "    regime_mae = [np.mean(regime_errors[k]) for k in sorted(regime_errors.keys())]\n",
    "    \n",
    "    bars = ax.bar(regime_labels, regime_mae, color=['green', 'yellow', 'red'][:len(regime_labels)], alpha=0.7)\n",
    "    ax.set_ylabel('Mean Absolute Error')\n",
    "    ax.set_title('Model Performance by Market Regime (Fold 1 Test Set)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mae in zip(bars, regime_mae):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{mae:.4f}',\n",
    "                ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRegime-Specific Performance:\")\n",
    "    for regime_id in sorted(regime_errors.keys()):\n",
    "        print(f\"  Regime {regime_id}: MAE = {np.mean(regime_errors[regime_id]):.6f}\")\n",
    "else:\n",
    "    print(\"No regime labels available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path('outputs/walkforward_dlinear')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results dataframe\n",
    "results_df.to_csv(output_dir / 'fold_results.csv', index=False)\n",
    "print(f\"Saved results to {output_dir / 'fold_results.csv'}\")\n",
    "\n",
    "# Optionally save model from last fold\n",
    "model_path = output_dir / f'dlinear_fold_{num_folds}.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved model to {model_path}\")\n",
    "\n",
    "print(\"\\nAll outputs saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Walk-Forward Validation**: Realistic temporal splitting to avoid lookahead bias\n",
    "2. **Regime Detection**: Automatic clustering of market states for adaptive modeling\n",
    "3. **DLinear Architecture**: Efficient decomposition-based time series forecasting\n",
    "4. **Per-Fold Training**: Separate models per fold simulating production retraining\n",
    "5. **Comprehensive Evaluation**: Metrics across folds and regimes\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different regime clustering strategies (GMM, HDBSCAN)\n",
    "- Add probabilistic forecasting (multi-path predictions with CRPS loss)\n",
    "- Implement regime drift monitoring for live deployment\n",
    "- Try different DLinear kernel sizes for seasonal decomposition\n",
    "- Extend to multi-asset modeling with cross-asset features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}